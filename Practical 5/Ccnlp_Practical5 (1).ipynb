{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: nltk>=3.1 in /home/s_r/.local/lib/python3.8/site-packages (from textblob) (3.5)\n",
      "Requirement already satisfied: joblib in /home/s_r/.local/lib/python3.8/site-packages (from nltk>=3.1->textblob) (0.16.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3.8/site-packages (from nltk>=3.1->textblob) (7.0)\n",
      "Requirement already satisfied: regex in /home/s_r/.local/lib/python3.8/site-packages (from nltk>=3.1->textblob) (2020.7.14)\n",
      "Requirement already satisfied: tqdm in /home/s_r/.local/lib/python3.8/site-packages (from nltk>=3.1->textblob) (4.48.2)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/s_r/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.275, subjectivity=0.8194444444444444)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from textblob import TextBlob\n",
    "from textblob import Blobber\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "blob = TextBlob(\"This restaurant was great, but I'm not sure if I'll go there again\")\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'This': True, 'is': True, 'a': True, 'good': True, 'book': True}, 'pos']\n",
      "[{'This': True, 'is': True, 'an': True, 'awesome': True, 'book': True}, 'pos']\n",
      "[{'This': True, 'is': True, 'a': True, 'bad': True, 'book': True}, 'neg']\n",
      "[{'This': True, 'is': True, 'a': True, 'terrible': True, 'book': True}, 'neg']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def from_sent(sent):\n",
    "    return {word: True for word in nltk.word_tokenize(sent)}\n",
    "\n",
    "s1 = \"This is a good book\\n\"\n",
    "s2 = \"This is an awesome book\\n\"\n",
    "s3 = \"This is a bad book\\n\"\n",
    "s4 = \"This is a terrible book\\n\"\n",
    "\n",
    "training_data = [[from_sent(s1),'pos'],[from_sent(s2),'pos'],[from_sent(s3),'neg'],[from_sent(s4),'neg']]\n",
    "for t in training_data:\n",
    "    print(t)\n",
    "    \n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "model = NaiveBayesClassifier.train(training_data)\n",
    "model.classify(from_sent(\"This is a good article\"))\n",
    "\n",
    "model.classify(from_sent(\"This is a bad article\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /home/s_r/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/names.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import names\n",
    "nltk.download('names')\n",
    "labeled_names = ([(name,'male') for name in names.words('male.txt')] + [(name,'female') for name in names.words('female.txt')])\n",
    "import random\n",
    "random.shuffle(labeled_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter':word[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(gender_features(n),gender) for (n,gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:],featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(gender_features('Pranav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier,test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'k'              male : female =     45.2 : 1.0\n",
      "             last_letter = 'a'            female : male   =     34.4 : 1.0\n",
      "             last_letter = 'f'              male : female =     24.3 : 1.0\n",
      "             last_letter = 'v'              male : female =     11.2 : 1.0\n",
      "             last_letter = 'd'              male : female =     10.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package subjectivity to /home/s_r/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/subjectivity.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "nltk.download('subjectivity')\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "n_instances = 100\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "len(subj_docs), len(obj_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subj_docs = subj_docs[:80]\n",
    "test_subj_docs = subj_docs[80:100]\n",
    "train_obj_docs = obj_docs[:80]\n",
    "test_obj_docs = obj_docs[80:100]\n",
    "training_docs = train_subj_docs+train_obj_docs\n",
    "testing_docs = test_subj_docs+test_obj_docs\n",
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "len(unigram_feats)                   \n",
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.8\n",
      "F-measure [obj]: 0.8\n",
      "F-measure [subj]: 0.8\n",
      "Precision [obj]: 0.8\n",
      "Precision [subj]: 0.8\n",
      "Recall [obj]: 0.8\n",
      "Recall [subj]: 0.8\n"
     ]
    }
   ],
   "source": [
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)\n",
    "\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)\n",
    "\n",
    "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "    print('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/s_r/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.672, 'neu': 0.328, 'pos': 0.0, 'compound': -0.6249}\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentences = [\"Vader is smart and funny\"]\n",
    "sentences = [\"Vader is good and funny\",\"Vader is smart and funny\",\"Vader is very smart and funny\"]\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentence = \"Vader is worst\"\n",
    "ss1 = sid.polarity_scores(sentence)\n",
    "print(ss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.58, 'pos': 0.42, 'compound': 0.4404}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Vader is too good sometimes\"\n",
    "ss3 = sid.polarity_scores(sentence)\n",
    "print(ss3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis using IBM Watson Tone Analyzer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/s_r/.cache/pip/wheels/49/6d/cf/1d91261b96363da78bf9b02699fd2262e6b5dad179500690c1/ibm_watson-5.1.0-cp38-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.3 in /usr/lib/python3.8/site-packages (from ibm-watson>=4.6.0) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0,>=2.0 in /usr/lib/python3.8/site-packages (from ibm-watson>=4.6.0) (2.23.0)\n",
      "Processing /home/s_r/.cache/pip/wheels/49/1a/93/9c99ecd9fcfcdc862e4f4e61fc596db58f579fbb4c89da47b3/ibm_cloud_sdk_core-3.3.6-cp38-none-any.whl\n",
      "Collecting websocket-client==0.48.0\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/a1/72ef9aa26cfe1a75cee09fc1957e4723add9de098c15719416a1ee89386b/websocket_client-0.48.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/lib/python3.8/site-packages (from python-dateutil>=2.5.3->ibm-watson>=4.6.0) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet>=3.0.2 in /usr/lib/python3.8/site-packages (from requests<3.0,>=2.0->ibm-watson>=4.6.0) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna>=2.5 in /usr/lib/python3.8/site-packages (from requests<3.0,>=2.0->ibm-watson>=4.6.0) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: urllib3>=1.21.1 in /usr/lib/python3.8/site-packages (from requests<3.0,>=2.0->ibm-watson>=4.6.0) (1.25.8)\n",
      "Collecting PyJWT<3.0.0,>=2.0.0a1\n",
      "  Using cached https://files.pythonhosted.org/packages/b4/9b/8850f99027ed029af6828199cc87179eaccbbf1f9e6e373e7f0177d32dad/PyJWT-2.0.1-py3-none-any.whl\n",
      "Installing collected packages: PyJWT, ibm-cloud-sdk-core, websocket-client, ibm-watson\n",
      "Successfully installed PyJWT-2.0.1 ibm-cloud-sdk-core-3.3.6 ibm-watson-5.1.0 websocket-client-0.48.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade \"ibm-watson>=4.6.0\" --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understand emotions and communication style in text\n",
    "#Analyze emotions and tones in what people write online, like tweets or reviews. \n",
    "#Predict whether they are happy, sad, confident, and more\n",
    "# Enhance customer service-See if customers are satisfied or frustrated, and if agents are polite and sympathetic.\n",
    "#The IBM Watson™ Tone Analyzer service uses linguistic analysis to detect emotional and language tones in written text\n",
    "# Use the service to understand how your written communications are perceived and then to improve the tone of communications. \n",
    "#Businesses can use the service to learn the tone of their customers' communications \n",
    "#respond to each customer appropriately, or to understand and improve their customer conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze general tone\n",
    "#Use the general-purpose endpoint to analyze the tone of your input content. \n",
    "#The service analyzes the content for emotional and language tones. \n",
    "#The method always analyzes the tone of the full document; \n",
    "#by default, it also analyzes the tone of each individual sentence of the content.\n",
    "#You can submit no more than 128 KB of total input content and \n",
    "#no more than 1000 individual sentences in JSON, plain text, or HTML format. \n",
    "#The service analyzes the first 1000 sentences for document-level analysis \n",
    "#and only the first 100 sentences for sentence-level analysis.\n",
    "\n",
    "#Per the JSON specification, the default character encoding for JSON content is effectively always UTF-8; \n",
    "#per the HTTP specification, the default encoding for plain text and HTML is ISO-8859-1 (effectively, the ASCII character set). \n",
    "#When specifying a content type of plain text or HTML, include the charset parameter to indicate the character encoding of the input text;\n",
    "#for example: Content-Type: text/plain;charset=utf-8. \n",
    "#For text/html, the service removes HTML tags and analyzes only the textual content.\n",
    "\n",
    "#tone(self, tone_input, content_type=None, sentences=None, tones=None, content_language=None, accept_language=None, **kwargs)\n",
    "#ToneInput- Input for the general-purpose endpoint.\n",
    "#content_type - The type of the input.\n",
    "# sentences -Indicates whether the service is to return an analysis of each individual sentence in addition to its analysis of the full document. \n",
    "#tones-list of tones for which the service is to return its analysis of the input; the indicated tones apply both to the full document and to individual sentences of the document. \n",
    "#content_language-The language of the input text for the request\n",
    "#accept_language -The desired language of the response\n",
    "\n",
    "#ToneAnalysis -The tone analysis results for the input from the general-purpose endpoint.\n",
    "#document_tone - The results of the analysis for the full input content.\n",
    "#sentences_tone- An array of SentenceAnalysis objects that provides the results of the analysis for the individual sentences of the input content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ibm_watson import ToneAnalyzerV3\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "authenticator = IAMAuthenticator('MDa1gpEjiFn63BJsMSJGvzFjjOVJqBkrHwY3_y41EcfW')\n",
    "tone_analyzer = ToneAnalyzerV3(\n",
    "    version='2017-09-21',\n",
    "    authenticator=authenticator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_analyzer.set_service_url('https://api.eu-gb.tone-analyzer.watson.cloud.ibm.com/instances/96f955fa-df0a-45cf-9d40-cef4b3995d3b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"document_tone\": {\n",
      "    \"tones\": [\n",
      "      {\n",
      "        \"score\": 0.6165,\n",
      "        \"tone_id\": \"sadness\",\n",
      "        \"tone_name\": \"Sadness\"\n",
      "      },\n",
      "      {\n",
      "        \"score\": 0.829888,\n",
      "        \"tone_id\": \"analytical\",\n",
      "        \"tone_name\": \"Analytical\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"sentences_tone\": [\n",
      "    {\n",
      "      \"sentence_id\": 0,\n",
      "      \"text\": \"Team, I know that times are tough!\",\n",
      "      \"tones\": [\n",
      "        {\n",
      "          \"score\": 0.801827,\n",
      "          \"tone_id\": \"analytical\",\n",
      "          \"tone_name\": \"Analytical\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"sentence_id\": 1,\n",
      "      \"text\": \"Product sales have been disappointing for the past three quarters.\",\n",
      "      \"tones\": [\n",
      "        {\n",
      "          \"score\": 0.771241,\n",
      "          \"tone_id\": \"sadness\",\n",
      "          \"tone_name\": \"Sadness\"\n",
      "        },\n",
      "        {\n",
      "          \"score\": 0.687768,\n",
      "          \"tone_id\": \"analytical\",\n",
      "          \"tone_name\": \"Analytical\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"sentence_id\": 2,\n",
      "      \"text\": \"We have a competitive product, but we need to do a better job of selling it!\",\n",
      "      \"tones\": [\n",
      "        {\n",
      "          \"score\": 0.506763,\n",
      "          \"tone_id\": \"analytical\",\n",
      "          \"tone_name\": \"Analytical\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "text = 'Team, I know that times are tough! Product '\\\n",
    "    'sales have been disappointing for the past three '\\\n",
    "    'quarters. We have a competitive product, but we '\\\n",
    "    'need to do a better job of selling it!'\n",
    "\n",
    "tone_analysis = tone_analyzer.tone(\n",
    "    {'text': text},\n",
    "    content_type='application/json'\n",
    ").get_result()\n",
    "print(json.dumps(tone_analysis, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze customer-engagement tone\n",
    "#Use the customer-engagement endpoint to analyze the tone of customer service and customer support conversations. \n",
    "#For each utterance of a conversation, the method reports the most prevalent subset of the following seven tones: \n",
    "#sad, frustrated, satisfied, excited, polite, impolite, and sympathetic.\n",
    "\n",
    "#If you submit more than 50 utterances, the service returns a warning for the overall content and \n",
    "#analyzes only the first 50 utterances. \n",
    "#If you submit a single utterance that contains more than 500 characters, \n",
    "#the service returns an error for that utterance and does not analyze the utterance. \n",
    "#The request fails if all utterances have more than 500 characters. \n",
    "#Per the JSON specification, the default character encoding for JSON content is effectively always UTF-8.\n",
    "\n",
    "#tone_chat(self, utterances, content_language=None, accept_language=None, **kwargs)\n",
    "#utterances- An array of Utterance objects that provides the input content that the service is to analyze.\n",
    "#content_language-The language of the input text for the request\n",
    "#accept_language -The desired language of the response\n",
    "\n",
    "#UtteranceAnalyses- The results of the analysis for the utterances of the input content.\n",
    "#utterances_tone- An array of UtteranceAnalysis objects that provides the results for each utterance of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"utterances_tone\": [\n",
      "    {\n",
      "      \"utterance_id\": 0,\n",
      "      \"utterance_text\": \"Hello, I'm having a problem with your product.\",\n",
      "      \"tones\": [\n",
      "        {\n",
      "          \"score\": 0.686361,\n",
      "          \"tone_id\": \"polite\",\n",
      "          \"tone_name\": \"Polite\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"utterance_id\": 1,\n",
      "      \"utterance_text\": \"OK, let me know what's going on, please.\",\n",
      "      \"tones\": [\n",
      "        {\n",
      "          \"score\": 0.92724,\n",
      "          \"tone_id\": \"polite\",\n",
      "          \"tone_name\": \"Polite\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"utterance_id\": 2,\n",
      "      \"utterance_text\": \"Well, nothing is working :(\",\n",
      "      \"tones\": [\n",
      "        {\n",
      "          \"score\": 0.997795,\n",
      "          \"tone_id\": \"sad\",\n",
      "          \"tone_name\": \"Sad\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"utterance_id\": 3,\n",
      "      \"utterance_text\": \"Sorry to hear that.\",\n",
      "      \"tones\": [\n",
      "        {\n",
      "          \"score\": 0.730982,\n",
      "          \"tone_id\": \"polite\",\n",
      "          \"tone_name\": \"Polite\"\n",
      "        },\n",
      "        {\n",
      "          \"score\": 0.672499,\n",
      "          \"tone_id\": \"sympathetic\",\n",
      "          \"tone_name\": \"Sympathetic\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "utterances = [\n",
    "    {\n",
    "        \"text\": \"Hello, I'm having a problem with your product.\",\n",
    "        \"user\": \"customer\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"OK, let me know what's going on, please.\",\n",
    "        \"user\": \"agent\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Well, nothing is working :(\",\n",
    "        \"user\": \"customer\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Sorry to hear that.\",\n",
    "        \"user\": \"agent\"\n",
    "    }\n",
    "]\n",
    "\n",
    "utterance_analyses = tone_analyzer.tone_chat(utterances).get_result()\n",
    "print(json.dumps(utterance_analyses, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Sentiment Analysis with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /home/s_r/.local/lib/python3.8/site-packages (3.9.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3.8/site-packages (from tweepy) (1.14.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/lib/python3.8/site-packages (from tweepy) (2.23.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/lib/python3.8/site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: chardet>=3.0.2 in /usr/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: idna>=2.5 in /usr/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (2.9)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (1.25.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/s_r/.local/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive tweets percentage: 34.0 %\n",
      "Negative tweets percentage: 18.0 %\n",
      "Neutral tweets percentage: 48.0 %\n",
      "\n",
      "\n",
      "Positive tweets:\n",
      "RT @zoo_bear: Todays petrol prices proves Narendra Modi is successful in making this country a Vishwaguru. @ashokepandit\n",
      "RT @schaheid: For the first time the people of India know that China is occupying Indian territory and Modi can't do anything about it: Ind…\n",
      "Republic Day Parade is tribute to India's great socio-cultural heritage: PM Narendra Modi\n",
      "https://t.co/P7zqgpdpQs… https://t.co/nPVOy8Bogg\n",
      "Psn\n",
      "\n",
      "Republic Day Parade is tribute to India's great socio-cultural heritage: PM Narendra Modi\n",
      "https://t.co/gulIQMw9Ev via NaMo App\n",
      ".@BJP4Bengal party offices distributed as many as 400 invitation cards to @victoriamemkol, a programme organised by… https://t.co/AMfAi3qQ9T\n",
      "I am here to help you elect a govt of farmers, labourers, small &amp; medium business people, a govt that looks after i… https://t.co/zhMJlTfXPY\n",
      "RT @INCIndia: That's the first attack on foundation of this country that is built on respect of every single person, respect of every relig…\n",
      "RT @INCIndia: First action that Narendra Modi takes- demonetisation. Crushing blow to the weakest in India. He destroyed lives of millions…\n",
      "RT @tathagata2: not merely to the Bose family,divided as it is. It was decent of the Prime Minister not to insist on taking along whoever h…\n",
      "RT @ndtvfeed: Bangladesh PM Sheikh Hasina Thanks PM Modi For COVID-19 Vaccine Gift https://t.co/xda92D8r1c https://t.co/B1wxAjgJzz\n",
      "\n",
      "\n",
      "Negative tweets:\n",
      "RT @TimesNow: This just shows that @RahulGandhi has a 'phobia' of Narendra Modi &amp; RSS. Even Congress has stopped taking him seriously: @She…\n",
      "RT @yippeekiyay_dk: According to a report,  60% of Narendra Modi's followers are fake on Twitter. \n",
      "\n",
      "Rests are Women abusers, paid blue tick…\n",
      "Hard work never brings fatigue. It brings satisfaction!!\n",
      "- Narendra Modi \n",
      "#monday #mondaymotivation #positivity… https://t.co/RBKK7NjFnu\n",
      "RT @SushantSin: The Guardian view on India's farming revolt: a bitter harvest. There’s a growing backlash against Narendra Modi’s autocrati…\n",
      "RT @ANI: #WATCH | We'll not allow Narendra Modi to destroy the foundation of India...He doesn't understand that only Tamil people can decid…\n",
      "RT @IndianExpress: SAD president Sukhbir Singh Badal has called former ally BJP as the real 'tukde tukde gang' which is trying to push patr…\n",
      "RT @INCIndia: We will not allow Narendra Modi to destroy India's foundation. He thinks because he can blackmail Govt of Tamil Nadu, he can…\n",
      "RT @Elizatweetz: I am not at all surprised to know that around 61.3 % of PM Modi's followers are fake.\n",
      "\n",
      "It is a well-known fact that Narend…\n",
      "RT @AudaciousQuest_: Hello world, acc to Twitter 60% of Narendra Modi's followers are fake, that's 41 million fake out of 65 million follow…\n"
     ]
    }
   ],
   "source": [
    "class TwitterClient(object): \n",
    "    ''' \n",
    "    Generic Twitter Class for sentiment analysis. \n",
    "    '''\n",
    "    def __init__(self): \n",
    "        ''' \n",
    "        Class constructor or initialization method. \n",
    "        '''\n",
    "        # keys and tokens from the Twitter Dev Console \n",
    "        consumer_key = 'ohI8ofhxSh84sGU9pWvhOfEE8'\n",
    "        consumer_secret = 'dIskwpQ9K74GgvOmRNQkVdlSLKx8RaBnaQVvg5dqStMr0gqfBx'\n",
    "        access_token = '1304458089119100929-89douCsK85ZbD4dzH4KM5KTTBwtpsH'\n",
    "        access_token_secret = 'ogXgQnu4dWqVdJEJqyROtc2lENC6HTc29fudV8w8vLr8i'\n",
    "  \n",
    "        # attempt authentication \n",
    "        try: \n",
    "            # create OAuthHandler object \n",
    "            self.auth = OAuthHandler(consumer_key, consumer_secret) \n",
    "            # set access token and secret \n",
    "            self.auth.set_access_token(access_token, access_token_secret) \n",
    "            # create tweepy API object to fetch tweets \n",
    "            self.api = tweepy.API(self.auth) \n",
    "        except: \n",
    "            print(\"Error: Authentication Failed\") \n",
    "  \n",
    "    def clean_tweet(self, tweet): \n",
    "        ''' \n",
    "        Utility function to clean tweet text by removing links, special characters \n",
    "        using simple regex statements. \n",
    "        '''\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) \n",
    "  \n",
    "    def get_tweet_sentiment(self, tweet): \n",
    "        ''' \n",
    "        Utility function to classify sentiment of passed tweet \n",
    "        using textblob's sentiment method \n",
    "        '''\n",
    "        # create TextBlob object of passed tweet text \n",
    "        analysis = TextBlob(self.clean_tweet(tweet)) \n",
    "        # set sentiment \n",
    "        if analysis.sentiment.polarity > 0: \n",
    "            return 'positive'\n",
    "        elif analysis.sentiment.polarity == 0: \n",
    "            return 'neutral'\n",
    "        else: \n",
    "            return 'negative'\n",
    "  \n",
    "    def get_tweets(self, query, count = 10): \n",
    "        ''' \n",
    "        Main function to fetch tweets and parse them. \n",
    "        '''\n",
    "        # empty list to store parsed tweets \n",
    "        tweets = [] \n",
    "  \n",
    "        try: \n",
    "            # call twitter api to fetch tweets \n",
    "            fetched_tweets = self.api.search(q = query, count = count) \n",
    "  \n",
    "            # parsing tweets one by one \n",
    "            for tweet in fetched_tweets: \n",
    "                # empty dictionary to store required params of a tweet \n",
    "                parsed_tweet = {} \n",
    "  \n",
    "                # saving text of tweet \n",
    "                parsed_tweet['text'] = tweet.text \n",
    "                # saving sentiment of tweet \n",
    "                parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text) \n",
    "  \n",
    "                # appending parsed tweet to tweets list \n",
    "                if tweet.retweet_count > 0: \n",
    "                    # if tweet has retweets, ensure that it is appended only once \n",
    "                    if parsed_tweet not in tweets: \n",
    "                        tweets.append(parsed_tweet) \n",
    "                else: \n",
    "                    tweets.append(parsed_tweet) \n",
    "  \n",
    "            # return parsed tweets \n",
    "            return tweets \n",
    "  \n",
    "        except tweepy.TweepError as e: \n",
    "            # print error (if any) \n",
    "            print(\"Error : \" + str(e)) \n",
    "  \n",
    "def main(): \n",
    "    # creating object of TwitterClient Class \n",
    "    api = TwitterClient() \n",
    "    # calling function to get tweets \n",
    "    tweets = api.get_tweets(query = 'Narendra Modi', count = 200) \n",
    "  \n",
    "    # picking positive tweets from tweets \n",
    "    ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive'] \n",
    "    # percentage of positive tweets \n",
    "    print(\"Positive tweets percentage: {} %\".format(100*len(ptweets)/len(tweets))) \n",
    "    # picking negative tweets from tweets \n",
    "    ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative'] \n",
    "    # percentage of negative tweets \n",
    "    print(\"Negative tweets percentage: {} %\".format(100*len(ntweets)/len(tweets))) \n",
    "    # percentage of neutral tweets \n",
    "    print(\"Neutral tweets percentage: {} %\".format(100*(len(tweets) -(len( ntweets )+len( ptweets)))/len(tweets))) \n",
    "  \n",
    "    # printing first 5 positive tweets \n",
    "    print(\"\\n\\nPositive tweets:\") \n",
    "    for tweet in ptweets[:10]: \n",
    "        print(tweet['text']) \n",
    "  \n",
    "    # printing first 5 negative tweets \n",
    "    print(\"\\n\\nNegative tweets:\") \n",
    "    for tweet in ntweets[:10]: \n",
    "        print(tweet['text']) \n",
    "  \n",
    "if __name__ == \"__main__\": \n",
    "    # calling main function \n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
